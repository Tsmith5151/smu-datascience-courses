{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment Two\n",
    "\n",
    "### Team: Trace Smith and Robert Flamenbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "<a id=\"top\"></a>\n",
    "# Live Session Assignment Two\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (.ipynb file, remember to save it!!) or HTML of the rendered notebook before the end of class.\n",
    "\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Classification Data</a>\n",
    "* <a href=\"#using_trees\">Using Decision Trees - Gini</a>\n",
    "* <a href=\"#entropy\">Using Decision Trees - Entropy</a>\n",
    "* <a href=\"#multi\">Multi-way Splits</a>\n",
    "* <a href=\"#sklearn\">Decision Trees in Scikit-Learn</a>\n",
    "\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Classification Data\n",
    "\n",
    "- Notes: `ds.data` is a matrix of feature values and `ds.target` is a column vector of the class output (in our case, the hand written digit we want to classify). Each class is a number (0 through 9) that we want to classify as one of ten hand written digits. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape: (1797, 64)\n",
      "Target Shape: (1797,)\n",
      "Range of Target: 0 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "print('Features Shape:', ds.data.shape) # there are 1797 instances and 64 features per instance\n",
    "print('Target Shape:', ds.target.shape )\n",
    "print('Range of Target:', np.min(ds.target),np.max(ds.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"using_trees\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Using Decision Trees\n",
    "\n",
    "**Question 1:** For the digits dataset, what are the type(s) of the attributes? How many attributes are there? What do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pixels: 64\n",
      "Target Labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Pixels: {}'.format(ds['data'].shape[1]))\n",
    "print('Target Labels: {}'.format(ds['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Using the gini coefficient\n",
    "\n",
    "The gini coefficient for a **given split** is given by:\n",
    "\n",
    "\n",
    "$$Gini=\\sum_{t=1}^T \\frac{n_t}{N}gini(t)$$\n",
    "\n",
    "\n",
    "- Where $T$ is the total number of splits (2 for binary attributes), $n_t$ is the number of instances in node $t$ after splitting, and $N$ is the total number of instances in the parent node. $gini(t)$ is the gini index for each individual node that is created by the split and is given by:\n",
    "\n",
    "\n",
    "$$gini(t)=1-\\sum_{j=0}^{C-1} p(j|t)^2$$\n",
    "\n",
    "\n",
    "- Where $C$ is the total number of possible classes and $p(j|t)$ is the probability of class $j$ in node $t$ (i.e., $n_j==$ the count of instances belonging to class $j$ in node $t$, normalized by the total number of instances in node $t$).\n",
    "\n",
    "\n",
    "$$ p(j|t) = \\frac{n_j}{n_t}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Index: 0.490\n"
     ]
    }
   ],
   "source": [
    "def gini_index(classes_in_split):\n",
    "    \n",
    "    \"\"\"Return Gini Index for Split\"\"\"\n",
    "    \n",
    "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
    "    gini = 1\n",
    "    for c in np.unique(classes_in_split):\n",
    "        gini -= (np.sum(classes_in_split == c) / float(len(classes_in_split)))**2\n",
    "    return(gini)\n",
    "\n",
    "labels = ['A','A','A','A','B','B','B']\n",
    "print('Gini Index: {0:.3f}'.format(gini_index(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, the function is used calculate the gini for splitting the dataset on feature 28, with value 2.5. For this example, we create two separate tree nodes: the first node has all the `ds.target` labels when feature 28 is greater than 2.5, the second node has all the rows from `ds.target` where feature 28 is less than 2.5. The steps are outlined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini for Right Node of Split: 0.885\n",
      "Gini for Left Node of Split: 0.712\n"
     ]
    }
   ],
   "source": [
    "gini_r = gini_index(ds.target[feature28>2.5]) \n",
    "gini_l = gini_index(ds.target[feature28<=2.5])\n",
    "\n",
    "print('Gini for Right Node of Split:', round(gini_r,3))\n",
    "print('Gini for Left Node of Split:', round(gini_l,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Using the above values `gini_r` and `gini_l`. Calculate the combined Gini for the entire split. \n",
    "\n",
    "- We can modify the above gini_index function to compute the weighted average of the left and right splits. The code below shows this modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(classes_in_split,n_g,n_t):\n",
    "    \n",
    "    classes_in_split = np.reshape(classes_in_split,(len(classes_in_split),-1))\n",
    "    unique_classes = np.unique(classes_in_split)\n",
    "    score = 0\n",
    "    for c in unique_classes:\n",
    "        score += ((np.sum(classes_in_split == c) / float(len(classes_in_split)))**2)\n",
    "    #Compute Weighted Average\n",
    "    gini = (1 - score) * (n_g/n_t)\n",
    "    return(gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Now we want to know which is a better split:\n",
    "- `feature28` split on a value of `2.5`  \n",
    "- `feature28` split on a value of `10`.  \n",
    "\n",
    "Find the total gini of splitting on the threshold of 10 and compare it to the total gini of splitting on threshold of 2.5 (for feature 28 only). According to gini, which threshold is better for spliting on feature 28, `threshold=2.5` or `threshold=10.0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left Node Samples:  1398\n",
      "Right Node Samples:  399\n",
      "Total Gini of the Split for Threshold of 2.5 is: 0.8462\n",
      "\n",
      "Left Node Samples:  1043\n",
      "Right Node Samples:  754\n",
      "Total Gini of the Split for Threshold of 10 is: 0.8636\n",
      "\n",
      "This is not better than the split on 2.5\n"
     ]
    }
   ],
   "source": [
    "def split_feature(df,col_idx,threshold):\n",
    "\n",
    "    feature = ds.data[:,col_idx]\n",
    "\n",
    "    left_split = ds.target[feature > threshold]\n",
    "    right_split = ds.target[feature <= threshold]\n",
    "    \n",
    "    #Number of Samples\n",
    "    n_parent = feature.shape[0]\n",
    "    n_left = left_split.shape[0]\n",
    "    n_right = right_split.shape[0]\n",
    "    \n",
    "    print('\\nLeft Node Samples: ',n_left)\n",
    "    print('Right Node Samples: ',n_right)\n",
    "\n",
    "    #Compute Gini Index\n",
    "    gini_left = gini_index(left_split,n_left,n_parent)\n",
    "    gini_right = gini_index(right_split,n_right,n_parent)\n",
    "    \n",
    "    #Total Gini Index:\n",
    "    total = gini_left + gini_right\n",
    "\n",
    "    print('Total Gini of the Split for Threshold of {0} is: {1:.4f}'.format(threshold,total))\n",
    "\n",
    "split_feature(ds,28,2.5)\n",
    "split_feature(ds,28,10)\n",
    "\n",
    "print('\\nThis is not better than the split on 2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"entropy\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Entropy based splitting\n",
    "\n",
    "The calculated entropy for a node $t$ by:\n",
    "\n",
    "\n",
    "$$ Entropy(t) = -\\sum p(j|t) \\log p(j|t) $$\n",
    "\n",
    "\n",
    "Where $p(j|t)$ is the same as above. To combine Entropy measures from a set of nodes, t = {1,...,T} we use: \n",
    "\n",
    "\n",
    "$$Entropy_{split}=\\sum_{t=1}^T \\frac{n_t}{N}Entropy(t)$$ \n",
    "\n",
    "\n",
    "Where $n_t$ and $N$ are the same as defined above for the $Gini$. Information gain is calculated by subtracting the Entropy of the split from the Entropy of the parent node before splitting:\n",
    "\n",
    "\n",
    "$$InfoGain = Entropy(p)-Entropy_{split}$$\n",
    "\n",
    "\n",
    "Where $p$ is the parent node before splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "**Question 4:** Calculate the **information gain** of the split when the threshold is 2.5 on `feature28`. What is the value of the information gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left Node Samples:  1398\n",
      "Right Node Samples:  399\n",
      "Total Entropy of the Split for Threshold of 2.5 is: 2.0296\n",
      "\n",
      "**Information Gain of the Split for Threshold of 2.5: is 0.2728**\n"
     ]
    }
   ],
   "source": [
    "def information_gain(df,Y,col_idx,threshold):\n",
    "\n",
    "    #Feature 28\n",
    "    feature = ds.data[:,col_idx]\n",
    "\n",
    "    left_split = ds.target[feature > threshold]\n",
    "    right_split = ds.target[feature <= threshold]\n",
    "    \n",
    "    #Number of Samples\n",
    "    n_parent = feature.shape[0]\n",
    "    n_left = left_split.shape[0]\n",
    "    n_right = right_split.shape[0]\n",
    "\n",
    "    print('\\nLeft Node Samples: ',n_left)\n",
    "    print('Right Node Samples: ',n_right)\n",
    "\n",
    "    #Compute Entropy\n",
    "    entropy_left = get_entropy(left_split,n_left,n_parent)\n",
    "    entropy_right = get_entropy(right_split,n_right,n_parent)\n",
    "    \n",
    "    #Total Entropy:\n",
    "    total = entropy_left + entropy_right\n",
    "\n",
    "    print('Total Entropy of the Split for Threshold of {0} is: {1:.4f}'.format(threshold,total))\n",
    "\n",
    "    #Compute Entropy for Root Node\n",
    "    classes_in_split = Y #All class labels\n",
    "    entropy_root = 0\n",
    "    for c in np.unique(classes_in_split):\n",
    "        p = np.sum(classes_in_split == c) / float(len(classes_in_split))\n",
    "        entropy_root += -p*np.log(p)\n",
    "        \n",
    "    print('\\n**Information Gain of the Split for Threshold of {0}: is {1:.4f}**'.format(threshold,(entropy_root - total)))\n",
    "\n",
    "if __name__== '__main__':\n",
    "    Y = ds['target']\n",
    "    information_gain(ds,Y,28,2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** What is the information gain if the threshold is 10.0 on `feature28`? According to information gain, is it better to split on a threshold of 2.5 or 10? Does entropy give the same decision as gini for this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left Node Samples:  1043\n",
      "Right Node Samples:  754\n",
      "Total Entropy of the Split for Threshold of 10 is: 2.0929\n",
      "\n",
      "**Information Gain of the Split for Threshold of 10: is 0.2096**\n",
      "\n",
      "This is not better than the split on 2.5\n",
      "This is not the same as Gini Index\n"
     ]
    }
   ],
   "source": [
    "information_gain(ds,Y,28,10)\n",
    "\n",
    "print('\\nThis is not better than the split on 2.5')\n",
    "print('This is not the same as Gini Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"multi\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Information gain and multi-way splitting\n",
    "Now assume that we can use not just a binary split, but a three way split. \n",
    "\n",
    "**Question 6** What is the information gain if we split feature28 on two thesholds (three separate nodes corresponding to three branches from one node) \n",
    "- node left: `feature28<2.5`, \n",
    "- node middle: `2.5<=feature28<10`, and \n",
    "- node right: `10<=feature28`? \n",
    "\n",
    "Is the information gain better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left Node Samples:  399\n",
      "Right Node Samples:  1099\n",
      "Middle Node Samples:  299\n",
      "\n",
      "**Information Gain is 0.3172**\n"
     ]
    }
   ],
   "source": [
    "def root_entropy(Y):\n",
    "    \n",
    "    \"\"\"Compute Root Node Entropy\"\"\"\n",
    "    entropy = 0\n",
    "    for c in np.unique(Y):\n",
    "        p = np.sum(Y == c) / float(len(Y))\n",
    "        entropy += -p*np.log(p)\n",
    "    return(entropy)\n",
    "\n",
    "def information_gain_multi_split(df,Y,col_idx,threshold1,threshold2):\n",
    "\n",
    "    feature = df.data[:,col_idx]\n",
    "    \n",
    "    left_split = Y[feature < threshold1]\n",
    "    middle_split = Y[(threshold1 <= feature) & (feature < threshold2)]\n",
    "    right_split = Y[feature >= threshold2]\n",
    "    \n",
    "    #Number of Samples\n",
    "    n_parent = feature.shape[0]\n",
    "    n_left = left_split.shape[0]\n",
    "    n_right = right_split.shape[0]\n",
    "    n_middle = middle_split.shape[0]\n",
    "\n",
    "    print('\\nLeft Node Samples: ',n_left)\n",
    "    print('Right Node Samples: ',n_right)\n",
    "    print('Middle Node Samples: ',n_middle)\n",
    "\n",
    "    #Compute Entropy\n",
    "    entropy_left = get_entropy(left_split,n_left,n_parent)\n",
    "    entropy_right = get_entropy(right_split,n_right,n_parent)\n",
    "    entropy_middle = get_entropy(middle_split,n_middle,n_parent)\n",
    "    \n",
    "    #Total Entropy:\n",
    "    total = entropy_left + entropy_right + entropy_middle\n",
    "\n",
    "    print('\\n**Information Gain is {0:.4f}**'.format(root_entropy(Y) - total))\n",
    "        \n",
    "information_gain_multi_split(ds,Y,28,2.5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Should we normalize the quantity that we just calculated if we want to compare it to the information gain of a binary split? Why or Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yes, we should normalize information gain. One reason is that it prevents the creating of a large number of splits. We can normalize by taking the gain ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"sklearn\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Decision Trees in Scikit-Learn\n",
    "\n",
    "\n",
    "**Question 8**: What algorithm does scikit-learn use for creating decision trees (i.e., ID3, C4.5, C5.0, CART, MARS, CHAID, etc.)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART\n"
     ]
    }
   ],
   "source": [
    "print(\"CART\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Question 9**: Using the documentation, use scikit-learn to train a decision tree on the digits data. Calculate the accuracy on the training data. What is the accuracy? Did you expect the decision tree to have this kind of accuracy? Why or Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "I did/did not expect the accuracy score to be 100% since we trained and tested to the model on the same dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "def decision_tree(X,Y):\n",
    "    \n",
    "    \"\"\"Regression -- Decision Tree\"\"\"\n",
    "    \n",
    "    #Setup a Decision Tree Regressor\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X,Y)\n",
    "    y_pred = clf.predict(X)\n",
    "    print('Accuracy:', accuracy_score(y_pred,Y))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    X = ds['data']\n",
    "    Y = ds['target']\n",
    "    decision_tree(X,Y)\n",
    "\n",
    "print('I did/did not expect the accuracy score to be 100% since we trained and tested to the model on the same dataset.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Question 10**: Look at the other input parameters to the function `DecisionTreeClassifier` could any of them be used to help prevent the decision tree from overlearning on the data? \n",
    "\n",
    "Which variables might we use to control overfitting and how (explain why it might help to stop overfitting)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could perform GridSearch to determine the optimal value for the **'max_depth'** of the tree. In the situation where the max_depth of the decision tree is 1, the model would be underfitting as the learning value is restricted to one level of the decision tree and does not allow the training set to learn data adequately. A low complexity decision tree results in high bias. On the other hand, for a max_depth = 10, this would clearly illustrate a high variance and overfitting the data. In this case, the model has virtually memorized the training data but will not be expected to perform well with out-of-sample data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
