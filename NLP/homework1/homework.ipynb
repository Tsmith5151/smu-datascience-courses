{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS: 7337 - Natural Language Processing\n",
    "**Author: Trace Smith**\n",
    "**Homework Assignment: 1**\n",
    "________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Python (if you don’t have it already), and install NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.9\n",
      "  latest version: 4.6.14\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine.**\n",
    "\n",
    "- In the definition of lexical_diversity(), we specify a parameter named text . This parameter is a \"placeholder\" for the actual text whose lexical diversity we want to compute, and reoccurs in the block of code that will run when the function is used. Similarly, percentage() is defined to take two parameters, named count and total.\n",
    "\n",
    "\n",
    "- Source: Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit; Steven Bird, Ewan Klein, and Edward Loper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go to http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf), and obtain three texts (of different grade levels) from the “Graded Readers” section. Report the lexical diversity score of each. Explain whether the result was surprising.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'http://www.gutenberg.org/cache/epub/15659/pg15659.txt' #Second Reader; 139 kB\n",
    "url2 = 'http://www.gutenberg.org/cache/epub/15170/pg15170.txt' #Third Reader; 210 kB\n",
    "url3 = 'http://www.gutenberg.org/cache/epub/9078/pg9078.txt' #Fourth Reader; 669 kB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    doc = request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(doc.lower(),\"html.parser\").get_text()\n",
    "    return raw\n",
    "\n",
    "text1 = get_text(url1)\n",
    "text2 = get_text(url2)\n",
    "text3 = get_text(url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity for text1 0.0004271948008291781\n",
      "Lexical Diversity for text2 0.0002932701483574544\n",
      "Lexical Diversity for text3 0.00010367460775223229\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "for idx,text in enumerate([text1,text2,text3]):\n",
    "    print('Lexical Diversity for text%s' % (str(idx+1)), lexical_diversity(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interesting, the lexical diversity decreases from second to third grade reader while the fourth grade reader decreases in contrast to the third grade reader book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also compare the vocabulary size of the same three texts. Explain whether the result was surprising.**\n",
    "\n",
    "- It's noticeable that the file size for text3 is the largest and corresponds to a higher vocabulary corpus compared to the second and third grade readers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary for text1 4099\n",
      "Total Vocabulary for text2 4465\n",
      "Total Vocabulary for text3 14906\n"
     ]
    }
   ],
   "source": [
    "def vocab(text):\n",
    "    return set(nltk.word_tokenize(text))\n",
    "    \n",
    "for idx,text in enumerate([text1,text2,text3]):\n",
    "    print('Total Vocabulary for text%s' % (str(idx+1)), len(vocab(text))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself.**\n",
    "\n",
    "Given that a text document has a higher vocabulary does not always mean that the degree of difficulty when reading the book is higher. In order words, from the example demonstrated above, the total vocabulary of the document increases, while the overall lexical diversity (e.g. ratio of different unique word stems in the corpus) decreases as the grade reading level increases. Therefore, we can conclude that a large vocabulary does not always coincide with a larger lexical diversity. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
